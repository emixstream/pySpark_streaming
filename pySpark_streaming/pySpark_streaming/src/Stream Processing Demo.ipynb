{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming - Demo\n",
    "This notebook will guide us through some examples of **Spark Streaming API**. \n",
    "In particular, in this notebook Spark will interact with **Apache Kafka** (a message broker) and will consume a stream of messages.\n",
    "\n",
    "As for the Spark Demo, the **SparkSession** object must be created in order to be able to work with streams. Note that in order to use Kafka the right libraries must be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this cell creates a Spark Session object that is used to interact with Spark\n",
    "from pyspark.sql import SparkSession\n",
    "ss = SparkSession.builder \\\n",
    ".config('spark.jars.packages', 'org.apache.kafka:kafka_2.11:1.1.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4') \\\n",
    ".getOrCreate()\n",
    "ss.version\n",
    "# version 2.4.4 uses Scala 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite dataframe\n",
    "\n",
    "Spark Streaming API allows us to work with infinite streams and manage them as they were relational tables. In the following cell the SparkSession maps a stream of messages from Kafka (topic *my_topic* from the broker available at *kafka:9092*) onto a dataframe object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = 'kafka:9092'\n",
    "df=(ss\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\",brokers)\n",
    "  .option(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "  .option(\"subscribe\", \"my_topic\")\n",
    "  .option(\"startingOffsets\", \"earliest\") # read data from the beginning of the stream\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a Kafka message is icludes:\n",
    "\n",
    "- a key\n",
    "- a value\n",
    "- the name of the topic\n",
    "- the stream partition\n",
    "- the timestamp\n",
    "\n",
    "It can be easily checked using the `printSchema()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imposing a schema\n",
    "\n",
    "To handle the data stream, Spark needs to access the `value` field of every Kafka message. In this scenario, a message is a string in json format. The following cell define a data schema to be used to extract to message's fields of `df` and a new dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType, TimestampType, BooleanType\n",
    "mySchema = StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"firstname\", IntegerType(), True),\n",
    "            StructField(\"lastname\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"gender\", StringType(), True),\n",
    "            StructField(\"arrested\", BooleanType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"race\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the `value` field and apply the schema using the `from_json()` function. A new dataframe `df1` is created. Note that, since it is lazy, no action is undertaken and no process is depolyed at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df1 =(df.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS TIMESTAMP)\")\n",
    "  .select(from_json(\"value\", mySchema).alias(\"data\"), \"timestamp\")\n",
    "  .select(\"data.race\",\n",
    "          \"data.gender\", \n",
    "          \"data.lastname\", \n",
    "          \"data.firstname\", \n",
    "          \"data.arrested\", \n",
    "          \"data.age\", \"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the schema we can check that the structure of `df1` is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- firstname: integer (nullable = true)\n",
      " |-- arrested: boolean (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL over stream\n",
    "\n",
    "Spark SQL API applies also on infinite dataframes. In the following cell the `df1` dataframe is registered as table `mytable`. An sql query is also defined; note that the result is another dataframe: `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"mytable\")\n",
    "df2 =ss.sql('''\n",
    "SELECT age, count(age) from mytable group by age'''\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the stream data processing\n",
    "\n",
    "In the following cell, the streaming query from `df2` is started, a window of 15 seconds is set up. The query sink is the machine's memory.\n",
    "The output mode **complete** means each output data point is a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery1 = df2.writeStream \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"test\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .trigger(processingTime = \"15 seconds\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the output stream from memory using sql over the output stream, named `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 0\n",
      "+---+----------+\n",
      "|age|count(age)|\n",
      "+---+----------+\n",
      "| 31|       235|\n",
      "| 65|       249|\n",
      "| 53|       262|\n",
      "| 34|       254|\n",
      "| 28|       265|\n",
      "| 76|       250|\n",
      "| 26|       244|\n",
      "| 27|       268|\n",
      "| 44|       221|\n",
      "| 12|       266|\n",
      "| 22|       241|\n",
      "| 47|       276|\n",
      "| 52|       226|\n",
      "| 13|       248|\n",
      "| 16|       282|\n",
      "| 40|       237|\n",
      "| 20|       251|\n",
      "| 57|       270|\n",
      "| 54|       245|\n",
      "| 48|       280|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Batch number 1\n",
      "+---+----------+\n",
      "|age|count(age)|\n",
      "+---+----------+\n",
      "| 31|       236|\n",
      "| 65|       251|\n",
      "| 53|       265|\n",
      "| 34|       257|\n",
      "| 28|       265|\n",
      "| 76|       253|\n",
      "| 26|       247|\n",
      "| 27|       269|\n",
      "| 44|       222|\n",
      "| 12|       268|\n",
      "| 22|       243|\n",
      "| 47|       279|\n",
      "| 52|       227|\n",
      "| 13|       251|\n",
      "| 16|       286|\n",
      "| 40|       239|\n",
      "| 20|       254|\n",
      "| 57|       273|\n",
      "| 54|       248|\n",
      "| 48|       281|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Batch number 2\n",
      "+---+----------+\n",
      "|age|count(age)|\n",
      "+---+----------+\n",
      "| 31|       236|\n",
      "| 65|       252|\n",
      "| 53|       268|\n",
      "| 34|       257|\n",
      "| 28|       268|\n",
      "| 76|       256|\n",
      "| 26|       248|\n",
      "| 27|       272|\n",
      "| 44|       224|\n",
      "| 12|       268|\n",
      "| 22|       243|\n",
      "| 47|       282|\n",
      "| 52|       228|\n",
      "| 13|       252|\n",
      "| 16|       289|\n",
      "| 40|       241|\n",
      "| 20|       257|\n",
      "| 57|       278|\n",
      "| 54|       252|\n",
      "| 48|       282|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Batch number 3\n",
      "+---+----------+\n",
      "|age|count(age)|\n",
      "+---+----------+\n",
      "| 31|       238|\n",
      "| 65|       252|\n",
      "| 53|       272|\n",
      "| 34|       258|\n",
      "| 28|       268|\n",
      "| 76|       256|\n",
      "| 26|       251|\n",
      "| 27|       277|\n",
      "| 44|       230|\n",
      "| 12|       270|\n",
      "| 22|       245|\n",
      "| 47|       283|\n",
      "| 52|       228|\n",
      "| 13|       254|\n",
      "| 16|       293|\n",
      "| 40|       243|\n",
      "| 20|       259|\n",
      "| 57|       278|\n",
      "| 54|       254|\n",
      "| 48|       284|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Batch number 4\n",
      "+---+----------+\n",
      "|age|count(age)|\n",
      "+---+----------+\n",
      "| 31|       238|\n",
      "| 65|       252|\n",
      "| 53|       273|\n",
      "| 34|       259|\n",
      "| 28|       271|\n",
      "| 76|       256|\n",
      "| 26|       253|\n",
      "| 27|       280|\n",
      "| 44|       231|\n",
      "| 12|       274|\n",
      "| 22|       246|\n",
      "| 47|       286|\n",
      "| 52|       230|\n",
      "| 13|       259|\n",
      "| 16|       295|\n",
      "| 40|       243|\n",
      "| 20|       265|\n",
      "| 57|       279|\n",
      "| 54|       255|\n",
      "| 48|       286|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    print(f'Batch number {i}')\n",
    "    ss.sql('select * from test').show()\n",
    "    time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the streaming query\n",
    "\n",
    "The just created application would run potentially forever. To stop it we can use the `stop()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update mode and Console output format\n",
    "\n",
    "The following cell deploy the previous stream data analysis by outputting the resulting stream in console in `update` output mode.\n",
    "In the update output mode only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates\n",
    "\n",
    "Use the `docker logs -f pyspark-jupyter` command to check that the application is running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery2=df2.writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .format(\"console\") \\\n",
    "  .trigger(processingTime = \"15 seconds\")  \\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the query is active with the `isActive` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery2.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop the application and check that it is not active anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery2.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging infinite columns\n",
    "\n",
    "The next cell generates a new dataframe (`df3`) concatenating the two columns of the `df2` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit \n",
    "df3=df2.select(concat(col(\"age\"), lit(\" \"), col(\"count(age)\")).alias(\"value\"))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinking into Kafka\n",
    "\n",
    "The next cell deploys the streaming application defined by `df3` using Kafka as sink. The `topic_out` is used as sink and the mode is `complete`. Note that the stream is checkpointed.\n",
    "\n",
    "To check the output stream use the `peeping_data_stream_out.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery3=df3 \\\n",
    "        .writeStream \\\n",
    "          .format(\"kafka\") \\\n",
    "          .outputMode(\"complete\") \\\n",
    "          .option(\"kafka.bootstrap.servers\",brokers) \\\n",
    "          .option(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") \\\n",
    "          .option(\"topic\", \"topic_out\") \\\n",
    "          .option(\"failOnDataLoss\",\"false\")\\\n",
    "          .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "          .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook concludes by stopping the active streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery3.isActive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
